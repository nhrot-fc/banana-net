{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9444640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6498782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = os.path.join(os.getcwd(), '..')\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, 'data')\n",
    "RAW_DATA_SUBDIR = os.path.join(DATA_DIR, 'raw')\n",
    "PROCESSED_DATA_SUBDIR = os.path.join(DATA_DIR, 'processed')\n",
    "sys.path.append(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6380d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EXAMPLE ANNOTATION FILE FORMAT:\n",
    "\n",
    "Selection\tView\tChannel\tBegin Time (s)\tEnd Time (s)\tLow Freq (Hz)\tHigh Freq (Hz)\tInband Power (dB FS)\tSpecies\tCall type\tRating\tReference\n",
    "1\tSpectrogram 1\t1\t1.789365731\t1.892598370\t5401.800\t7438.600\t-29.01\tLW\tAA\tA\t\n",
    "2\tSpectrogram 1\t1\t2.691422357\t2.794654995\t5401.800\t7438.600\t-60.31\t\tnoise\t\t1\n",
    "3\tSpectrogram 1\t1\t20.557990213\t20.631727812\t6110.200\t8589.800\t-33.82\tLW\tAA\tA\t\n",
    "4\tSpectrogram 1\t1\t26.138457751\t26.212195350\t6110.200\t8589.800\t-58.35\t\tnoise\t\t3\n",
    "\n",
    "COLUMNS TO KEEP:\n",
    "Begin Time (s), End Time (s), Low Freq (Hz), High Freq (Hz), Inband Power (dB FS), Species, Call type, Rating\n",
    "\"\"\"\n",
    "SELECTED_CALL_DIR = os.path.join(RAW_DATA_SUBDIR, \"weddells_saddleBack_tamarin__LW\")\n",
    "\n",
    "# Get the list of recording files\n",
    "recordings_files = [file for file in os.listdir(SELECTED_CALL_DIR) if file.lower().endswith('.wav')]\n",
    "recordings_files.sort()\n",
    "# Get the list of annotation files\n",
    "annotations_files = [file for file in os.listdir(SELECTED_CALL_DIR) if file.lower().endswith('.txt')]\n",
    "annotations_files.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d559d98e",
   "metadata": {},
   "source": [
    "## Using the New Parsing Utility Functions\n",
    "\n",
    "The code above has been refactored into reusable functions in the `parsing.py` module. Here's how to use them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the parsing utilities\n",
    "from src.banana_net.utils.parsing import (\n",
    "    load_and_process_annotations,\n",
    "    print_dataset_summary,\n",
    "    plot_call_type_distribution,\n",
    "    save_processed_dataset\n",
    ")\n",
    "\n",
    "# Process the same data using the new functions\n",
    "SELECTED_CALL_DIR = os.path.join(RAW_DATA_SUBDIR, \"weddells_saddleBack_tamarin__LW\")\n",
    "\n",
    "# Load and process all annotations with the pipeline function\n",
    "processed_dataset = load_and_process_annotations(\n",
    "    directory=SELECTED_CALL_DIR,\n",
    "    filter_top_n=5  # Keep only top 3 most common call types\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print_dataset_summary(processed_dataset)\n",
    "\n",
    "# Plot distribution\n",
    "plot_save_path = os.path.join(PROCESSED_DATA_SUBDIR, 'call_type_distribution_new.png')\n",
    "plot_call_type_distribution(processed_dataset, save_path=plot_save_path)\n",
    "\n",
    "# Save the processed dataset\n",
    "save_path = os.path.join(PROCESSED_DATA_SUBDIR, 'call_dataset_new.csv')\n",
    "save_processed_dataset(processed_dataset, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24050cf4",
   "metadata": {},
   "source": [
    "## Transform to YOLO y-target Tensor\n",
    "\n",
    "### Example: Process Weddell's Tamarin data\n",
    "\n",
    "| index | begin_time | end_time\t| low_freq\t| high_freq\t| inband_power\t| species\t| call_type |\trecording_file      |\n",
    "|-------|------------|----------|-----------|-----------|---------------|-----------|-----------|-----------------------|\n",
    "| 0\t    | 1.336330\t | 1.865889\t| 7560.000\t| 10080.000 |\t-46.04\t    | lw\t    | cs        |\t20240117_162607.wav |\n",
    "| 1\t    | 1.950137\t | 2.449607\t| 7766.599\t| 10801.822 |\t-41.65\t    | lw\t    | cs        |\t20240117_162607.wav |\n",
    "| 2\t    | 2.527837\t | 3.057396\t| 8212.955\t| 10444.737 |\t-41.20\t    | lw\t    | cs        |\t20240117_162607.wav |\n",
    "| 3\t    | 3.135626\t | 3.611026\t| 8034.413\t| 10266.194 |\t-40.34\t    | lw\t    | cs        |\t20240117_162607.wav |\n",
    "| 4\t    | 3.767486\t | 4.182708\t| 7588.057\t| 9552.024  |\t-41.43\t    | lw\t    | cs        |\t20240117_162607.wav |\n",
    "\n",
    "YOLO-like tensor format:\n",
    "Our system models detection as a regres-\n",
    "sion problem. It divides the image into an S x S grid and for each\n",
    "grid cell predicts B bounding boxes, confidence for those boxes,\n",
    "and C class probabilities. These predictions are encoded as an\n",
    "S x S x (B * 5 | C) tensor.\n",
    "\n",
    "The 5 in the B * 5 term corresponds to the bounding box coordinates (x, y, width, height) and the confidence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168cfc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "CALL_CSV = os.path.join(PROCESSED_DATA_SUBDIR, 'call_dataset_new.csv')\n",
    "\n",
    "# Load the processed dataset for further analysis\n",
    "call_data = pd.read_csv(CALL_CSV)\n",
    "\n",
    "# Describe duratio and frequency range for each call type\n",
    "def describe_call_types(call_data):\n",
    "    call_types = call_data['call_type'].unique()\n",
    "    descriptions = {}\n",
    "\n",
    "    for call_type in call_types:\n",
    "        type_data = call_data[call_data['call_type'] == call_type]\n",
    "        # Calculate average duration, low frequency, and high frequency\n",
    "        duration = (type_data['end_time'] - type_data['begin_time']).mean()\n",
    "        low_freq = type_data['low_freq'].mean()\n",
    "        high_freq = type_data['high_freq'].mean()\n",
    "        # Add minimum and maximum duration, low frequency, and high frequency\n",
    "        min_duration = (type_data['end_time'] - type_data['begin_time']).min()\n",
    "        max_duration = (type_data['end_time'] - type_data['begin_time']).max()\n",
    "        min_low_freq = type_data['low_freq'].min()\n",
    "        max_low_freq = type_data['low_freq'].max()\n",
    "        min_high_freq = type_data['high_freq'].min()\n",
    "        max_high_freq = type_data['high_freq'].max()\n",
    "        # Store the results in the descriptions dictionary\n",
    "        descriptions[call_type] = {\n",
    "            'average_duration': duration,\n",
    "            'min_duration': min_duration,\n",
    "            'max_duration': max_duration,\n",
    "            'average_low_frequency': low_freq,\n",
    "            'min_low_frequency': min_low_freq,\n",
    "            'max_low_frequency': max_low_freq,\n",
    "            'average_high_frequency': high_freq,\n",
    "            'min_high_frequency': min_high_freq,\n",
    "            'max_high_frequency': max_high_freq\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame(descriptions).T\n",
    "# Get the descriptions of call types\n",
    "call_type_descriptions = describe_call_types(call_data)\n",
    "# Print the call type descriptions\n",
    "print(\"\\nCall Type Descriptions:\")\n",
    "print(call_type_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8923f284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop cc call type\n",
    "call_data = call_data[call_data['call_type'] != 'cc']\n",
    "# Save the cleaned dataset without 'cc' call type\n",
    "cleaned_save_path = os.path.join(PROCESSED_DATA_SUBDIR, 'call_dataset_cleaned.csv')\n",
    "save_processed_dataset(call_data, cleaned_save_path)\n",
    "# Print the cleaned dataset summary\n",
    "print(\"\\nCleaned Dataset Summary:\")\n",
    "print_dataset_summary(call_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23af6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.banana_net.utils.audio_clip_processing import (\n",
    "    process_dataset_to_clips,\n",
    ")\n",
    "\n",
    "# Process the dataset into audio clips\n",
    "clips_dir = os.path.join(PROCESSED_DATA_SUBDIR, 'audio_clips')\n",
    "\n",
    "class_map = {\n",
    "    ('lw', 'cs'): 0,\n",
    "    ('lw', 'tr'): 2,\n",
    "    ('lw', 'ta'): 2,\n",
    "    ('lw', 'tj'): 2,\n",
    "}\n",
    "\n",
    "all_clips, all_tensors = process_dataset_to_clips(\n",
    "    df = call_data,\n",
    "    clip_duration = 5.0, \n",
    "    overlap = 1.0, \n",
    "    max_freq_hz = 24000.0, \n",
    "    S = 7, \n",
    "    B = 2, \n",
    "    class_map = class_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed948b97",
   "metadata": {},
   "source": [
    "## Visualizing Spectrograms with Bounding Boxes\n",
    "\n",
    "Ahora vamos a verificar si los tensores tienen la forma correcta visualizando el espectrograma de algunos clips y dibujando los bounding boxes de los eventos detectados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f7845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, List\n",
    "from src.banana_net.utils.audio_clip_processing import AudioClip\n",
    "\n",
    "# Get a sample clip\n",
    "sample_clip_name = list(all_tensors.keys())[0]\n",
    "sample_tensor = all_tensors[sample_clip_name]\n",
    "sample_clip = next(clip for clip in all_clips if clip.clip_name == sample_clip_name)\n",
    "\n",
    "print(f\"Sample clip: {sample_clip_name}\")\n",
    "print(f\"Tensor shape: {sample_tensor.shape}\")\n",
    "print(f\"Number of annotations: {sample_clip.num_annotations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f3c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_path(clip_name: str) -> str:\n",
    "    \"\"\"Get the file path for an audio clip.\"\"\"\n",
    "    recording_file = clip_name.split('_clip_')[0]\n",
    "    audio_path = os.path.join(RAW_DATA_SUBDIR, \"weddells_saddleBack_tamarin__LW\", f\"{recording_file}\")\n",
    "    return audio_path\n",
    "\n",
    "def load_and_extract_clip(clip: AudioClip) -> np.ndarray:\n",
    "    \"\"\"Load audio file and extract the clip segment.\"\"\"\n",
    "    audio_path = get_audio_path(clip.original_file)\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, sr=None)\n",
    "        start_sample = int(clip.start_time * sr)\n",
    "        end_sample = int(clip.end_time * sr)\n",
    "        clip_audio = y[start_sample:end_sample]\n",
    "        return clip_audio, sr\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio: {e}\")\n",
    "        # Return a dummy signal\n",
    "        return np.zeros(22050 * 5), 22050\n",
    "\n",
    "def decode_yolo_tensor(tensor: np.ndarray, S: int, B: int, class_map: Dict) -> List[dict]:\n",
    "    \"\"\"Decode YOLO tensor to get bounding box parameters.\"\"\"\n",
    "    boxes = []\n",
    "    classes_map_reverse = {v: k for k, v in class_map.items()}\n",
    "    \n",
    "    for row in range(S):\n",
    "        for col in range(S):\n",
    "            for b in range(B):\n",
    "                box_offset = b * 5\n",
    "                confidence = tensor[row, col, box_offset + 4]\n",
    "                \n",
    "                if confidence > 0.5:  # Confidence threshold\n",
    "                    x_cell, y_cell, w, h = tensor[row, col, box_offset:box_offset + 4]\n",
    "                    \n",
    "                    # Calculate center coordinates relative to the entire grid\n",
    "                    center_x = (col + x_cell) / S\n",
    "                    center_y = (row + y_cell) / S\n",
    "                    \n",
    "                    # Class probabilities start after all bounding boxes\n",
    "                    class_offset = B * 5\n",
    "                    class_probabilities = tensor[row, col, class_offset:]\n",
    "                    class_id = np.argmax(class_probabilities)\n",
    "                    \n",
    "                    if classes_map_reverse.get(class_id) is not None:\n",
    "                        species, call_type = classes_map_reverse[class_id]\n",
    "                    else:\n",
    "                        species, call_type = \"unknown\", \"unknown\"\n",
    "                    \n",
    "                    boxes.append({\n",
    "                        \"center_x\": center_x,\n",
    "                        \"center_y\": center_y,\n",
    "                        \"width\": w,\n",
    "                        \"height\": h,\n",
    "                        \"confidence\": confidence,\n",
    "                        \"class_id\": class_id,\n",
    "                        \"species\": species,\n",
    "                        \"call_type\": call_type\n",
    "                    })\n",
    "    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a5b553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_spectrogram_with_boxes(clip: AudioClip, tensor: np.ndarray, max_freq_hz: float = 24000.0, \n",
    "                                  S: int = 7, B: int = 2, class_map: Dict = None):\n",
    "    \"\"\"Visualize spectrogram with bounding boxes from YOLO tensor.\"\"\"\n",
    "    # Load audio clip\n",
    "    audio_data, sr = load_and_extract_clip(clip)\n",
    "    \n",
    "    # Create spectrogram\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot spectrogram\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_data)), ref=np.max)\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', cmap='viridis')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(f'Spectrogram: {clip.clip_name}')\n",
    "    \n",
    "    # Plot spectrogram with bounding boxes\n",
    "    plt.subplot(2, 1, 2)\n",
    "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', cmap='viridis', vmax=0)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    \n",
    "    # Get bounding boxes from tensor\n",
    "    boxes = decode_yolo_tensor(tensor, S, B, class_map)\n",
    "    print(f\"Found {len(boxes)} boxes in tensor\")\n",
    "    \n",
    "    # Create plot axes for adding rectangles\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    # Draw boxes\n",
    "    for box in boxes:\n",
    "        # Convert normalized coordinates to time and frequency\n",
    "        center_time = box[\"center_x\"] * clip.duration\n",
    "        center_freq = box[\"center_y\"] * max_freq_hz\n",
    "        width_time = box[\"width\"] * clip.duration\n",
    "        height_freq = box[\"height\"] * max_freq_hz\n",
    "        \n",
    "        # Calculate rectangle parameters (lower left corner, width, height)\n",
    "        rect_time = center_time - (width_time / 2)\n",
    "        rect_freq = center_freq - (height_freq / 2)\n",
    "        \n",
    "        # Create rectangle\n",
    "        rect = patches.Rectangle(\n",
    "            (rect_time, rect_freq), width_time, height_freq,\n",
    "            linewidth=2, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        \n",
    "        # Add rectangle to plot\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add label above the box\n",
    "        label = f\"{box['species']}-{box['call_type']} ({box['confidence']:.2f})\"\n",
    "        plt.text(center_time, center_freq + height_freq/2 + 500, label, \n",
    "                 color='white', fontsize=10, bbox=dict(facecolor='red', alpha=0.5))\n",
    "    \n",
    "    plt.title(f'Spectrogram with Annotations: {len(boxes)} detections')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a988359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample clip with bounding boxes\n",
    "boxes = visualize_spectrogram_with_boxes(\n",
    "    sample_clip, \n",
    "    sample_tensor, \n",
    "    max_freq_hz=24000.0, \n",
    "    S=7, \n",
    "    B=2,\n",
    "    class_map=class_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936fb5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multiple_clips(num_clips: int = 3):\n",
    "    \"\"\"Visualize multiple clips to verify tensor correctness.\"\"\"\n",
    "    # Get clips with annotations\n",
    "    clips_with_annotations = [clip for clip in all_clips if clip.num_annotations > 0]\n",
    "    \n",
    "    # Select a few random clips\n",
    "    import random\n",
    "    if clips_with_annotations:\n",
    "        selected_clips = random.sample(clips_with_annotations, min(num_clips, len(clips_with_annotations)))\n",
    "        \n",
    "        for clip in selected_clips:\n",
    "            tensor = all_tensors[clip.clip_name]\n",
    "            boxes = visualize_spectrogram_with_boxes(\n",
    "                clip, \n",
    "                tensor, \n",
    "                max_freq_hz=24000.0, \n",
    "                S=7, \n",
    "                B=2,\n",
    "                class_map=class_map\n",
    "            )\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No clips with annotations found.\")\n",
    "\n",
    "# Visualize multiple clips\n",
    "visualize_multiple_clips(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715e5bcc",
   "metadata": {},
   "source": [
    "## Comparación de Anotaciones Originales vs Representación Tensorial\n",
    "\n",
    "Para verificar si los tensores tienen la forma correcta, vamos a comparar las anotaciones originales con la representación tensorial para un clip específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f46e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_original_annotations_with_tensor(clip, tensor, max_freq_hz=24000.0, S=7, B=2, class_map=None):\n",
    "    \"\"\"Compare original annotations with their tensor representation.\"\"\"\n",
    "    # Get the original annotations for this clip\n",
    "    original_annotations = clip.annotations.copy()\n",
    "    \n",
    "    # Decode the boxes from the tensor\n",
    "    decoded_boxes = decode_yolo_tensor(tensor, S, B, class_map)\n",
    "    \n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Load audio data for the spectrogram background\n",
    "    audio_data, sr = load_and_extract_clip(clip)\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_data)), ref=np.max)\n",
    "    \n",
    "    # Plot spectrogram with original annotations\n",
    "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', cmap='viridis', ax=ax1)\n",
    "    ax1.set_title(f'Original Annotations: {clip.clip_name}')\n",
    "    \n",
    "    # Draw original annotation boxes\n",
    "    for _, ann in original_annotations.iterrows():\n",
    "        begin_time_clip = ann['begin_time_clip']\n",
    "        end_time_clip = ann['end_time_clip']\n",
    "        low_freq = ann['low_freq']\n",
    "        high_freq = ann['high_freq']\n",
    "        species = ann['species']\n",
    "        call_type = ann['call_type']\n",
    "        \n",
    "        width = end_time_clip - begin_time_clip\n",
    "        height = high_freq - low_freq\n",
    "        \n",
    "        # Create rectangle for original annotation\n",
    "        rect = patches.Rectangle(\n",
    "            (begin_time_clip, low_freq), width, height,\n",
    "            linewidth=2, edgecolor='blue', facecolor='none'\n",
    "        )\n",
    "        ax1.add_patch(rect)\n",
    "        \n",
    "        # Add label\n",
    "        ax1.text(begin_time_clip + width/2, high_freq + 500, \n",
    "                f\"{species}-{call_type}\",\n",
    "                color='white', fontsize=9, ha='center',\n",
    "                bbox=dict(facecolor='blue', alpha=0.5))\n",
    "    \n",
    "    # Plot spectrogram with tensor-derived annotations\n",
    "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', cmap='viridis', ax=ax2)\n",
    "    ax2.set_title(f'Tensor-derived Annotations: {clip.clip_name}')\n",
    "    \n",
    "    # Draw tensor-derived annotation boxes\n",
    "    for box in decoded_boxes:\n",
    "        # Convert normalized coordinates to time and frequency\n",
    "        center_time = box[\"center_x\"] * clip.duration\n",
    "        center_freq = box[\"center_y\"] * max_freq_hz\n",
    "        width_time = box[\"width\"] * clip.duration\n",
    "        height_freq = box[\"height\"] * max_freq_hz\n",
    "        \n",
    "        # Calculate rectangle parameters\n",
    "        rect_time = center_time - (width_time / 2)\n",
    "        rect_freq = center_freq - (height_freq / 2)\n",
    "        \n",
    "        # Create rectangle for tensor-derived annotation\n",
    "        rect = patches.Rectangle(\n",
    "            (rect_time, rect_freq), width_time, height_freq,\n",
    "            linewidth=2, edgecolor='red', facecolor='none'\n",
    "        )\n",
    "        ax2.add_patch(rect)\n",
    "        \n",
    "        # Add label\n",
    "        ax2.text(center_time, center_freq + height_freq/2 + 500, \n",
    "                f\"{box['species']}-{box['call_type']} ({box['confidence']:.2f})\", \n",
    "                color='white', fontsize=9, ha='center',\n",
    "                bbox=dict(facecolor='red', alpha=0.5))\n",
    "    \n",
    "    # Add grid lines to visualize the YOLO grid cells\n",
    "    for i in range(1, S):\n",
    "        ax2.axhline(y=(i/S) * max_freq_hz, color='gray', linestyle=':', alpha=0.5)\n",
    "        ax2.axvline(x=(i/S) * clip.duration, color='gray', linestyle=':', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return original_annotations, decoded_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783777be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a clip with multiple annotations for comparison\n",
    "clips_with_multiple_annotations = [clip for clip in all_clips if clip.num_annotations >= 3]\n",
    "if clips_with_multiple_annotations:\n",
    "    comparison_clip = clips_with_multiple_annotations[0]  # Take first clip with multiple annotations\n",
    "    comparison_tensor = all_tensors[comparison_clip.clip_name]\n",
    "    \n",
    "    print(f\"Comparing annotations for clip: {comparison_clip.clip_name}\")\n",
    "    print(f\"Number of original annotations: {comparison_clip.num_annotations}\")\n",
    "    \n",
    "    original_anns, decoded_boxes = compare_original_annotations_with_tensor(\n",
    "        comparison_clip,\n",
    "        comparison_tensor,\n",
    "        max_freq_hz=24000.0,\n",
    "        S=7,\n",
    "        B=2,\n",
    "        class_map=class_map\n",
    "    )\n",
    "    \n",
    "    # Print detailed comparison\n",
    "    print(\"\\nOriginal annotations:\")\n",
    "    for i, ann in original_anns.iterrows():\n",
    "        print(f\"  {i+1}. {ann['species']}-{ann['call_type']}: Time [{ann['begin_time_clip']:.2f}-{ann['end_time_clip']:.2f}], Freq [{ann['low_freq']:.1f}-{ann['high_freq']:.1f}] Hz\")\n",
    "    \n",
    "    print(\"\\nTensor-decoded boxes:\")\n",
    "    for i, box in enumerate(decoded_boxes):\n",
    "        center_time = box[\"center_x\"] * comparison_clip.duration\n",
    "        center_freq = box[\"center_y\"] * 24000.0\n",
    "        width_time = box[\"width\"] * comparison_clip.duration\n",
    "        height_freq = box[\"height\"] * 24000.0\n",
    "        t_min = center_time - width_time/2\n",
    "        t_max = center_time + width_time/2\n",
    "        f_min = center_freq - height_freq/2\n",
    "        f_max = center_freq + height_freq/2\n",
    "        print(f\"  {i+1}. {box['species']}-{box['call_type']}: Time [{t_min:.2f}-{t_max:.2f}], Freq [{f_min:.1f}-{f_max:.1f}] Hz (conf: {box['confidence']:.2f})\")\n",
    "else:\n",
    "    print(\"No clips with multiple annotations found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d099aae",
   "metadata": {},
   "source": [
    "## Visualización del Grid YOLO\n",
    "\n",
    "Para entender mejor cómo funciona la representación YOLO, visualizamos el grid S×S y cómo las anotaciones se asignan a las celdas del grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3887cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_yolo_grid(clip, tensor, max_freq_hz=24000.0, S=7, B=2, class_map=None):\n",
    "    \"\"\"Visualize the YOLO grid structure and how annotations are assigned to grid cells.\"\"\"\n",
    "    # Load audio data\n",
    "    audio_data, sr = load_and_extract_clip(clip)\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_data)), ref=np.max)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot spectrogram\n",
    "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', cmap='viridis')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(f'YOLO Grid Visualization ({S}×{S} grid): {clip.clip_name}')\n",
    "    \n",
    "    # Get axis for drawing\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    # Draw grid lines\n",
    "    grid_colors = ['white', 'yellow', 'cyan', 'magenta']\n",
    "    \n",
    "    # Draw horizontal and vertical grid lines\n",
    "    for i in range(S+1):\n",
    "        # Horizontal lines (frequency divisions)\n",
    "        y_pos = (i/S) * max_freq_hz\n",
    "        ax.axhline(y=y_pos, color='white', linestyle='-', alpha=0.5)\n",
    "        \n",
    "        # Vertical lines (time divisions)\n",
    "        x_pos = (i/S) * clip.duration\n",
    "        ax.axvline(x=x_pos, color='white', linestyle='-', alpha=0.5)\n",
    "        \n",
    "        # Add grid cell coordinates\n",
    "        if i < S:\n",
    "            for j in range(S):\n",
    "                cell_center_x = (j + 0.5) * (clip.duration / S)\n",
    "                cell_center_y = (i + 0.5) * (max_freq_hz / S)\n",
    "                ax.text(cell_center_x, cell_center_y, f\"({j},{i})\", \n",
    "                        color='white', fontsize=9, ha='center', va='center',\n",
    "                        bbox=dict(facecolor='black', alpha=0.5))\n",
    "    \n",
    "    # Decode and visualize the boxes from tensor\n",
    "    boxes = decode_yolo_tensor(tensor, S, B, class_map)\n",
    "    \n",
    "    # Draw boxes with different colors based on which grid cell they belong to\n",
    "    for box in boxes:\n",
    "        # Convert normalized coordinates to time and frequency\n",
    "        center_time = box[\"center_x\"] * clip.duration\n",
    "        center_freq = box[\"center_y\"] * max_freq_hz\n",
    "        width_time = box[\"width\"] * clip.duration\n",
    "        height_freq = box[\"height\"] * max_freq_hz\n",
    "        \n",
    "        # Calculate rectangle parameters\n",
    "        rect_time = center_time - (width_time / 2)\n",
    "        rect_freq = center_freq - (height_freq / 2)\n",
    "        \n",
    "        # Determine which grid cell this belongs to\n",
    "        grid_col = int(box[\"center_x\"] * S)\n",
    "        grid_row = int(box[\"center_y\"] * S)\n",
    "        color_idx = (grid_row + grid_col) % len(grid_colors)\n",
    "        \n",
    "        # Create rectangle\n",
    "        rect = patches.Rectangle(\n",
    "            (rect_time, rect_freq), width_time, height_freq,\n",
    "            linewidth=2, edgecolor=grid_colors[color_idx], facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add label indicating grid cell assignment\n",
    "        label = f\"{box['species']}-{box['call_type']} (cell: {grid_col},{grid_row})\"\n",
    "        plt.text(center_time, center_freq + height_freq/2 + 500, label,\n",
    "                 color='white', fontsize=9, ha='center',\n",
    "                 bbox=dict(facecolor=grid_colors[color_idx], alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326cd0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the YOLO grid for the sample clip\n",
    "if clips_with_multiple_annotations:\n",
    "    grid_boxes = visualize_yolo_grid(\n",
    "        comparison_clip,\n",
    "        comparison_tensor,\n",
    "        max_freq_hz=24000.0,\n",
    "        S=7,\n",
    "        B=2,\n",
    "        class_map=class_map\n",
    "    )\n",
    "    \n",
    "    # Print information about tensor shape and format\n",
    "    print(f\"\\nYOLO tensor format explanation:\")\n",
    "    print(f\"- Grid size (S): 7x7\")\n",
    "    print(f\"- Bounding boxes per cell (B): 2\")\n",
    "    print(f\"- Number of classes (C): {len(class_map)}\")\n",
    "    print(f\"- Tensor shape: {comparison_tensor.shape} = (S, S, B*5 + C) = (7, 7, {2*5 + len(class_map)})\")\n",
    "    print(\"- Each bounding box has 5 values: (x, y, width, height, confidence)\")\n",
    "    \n",
    "    # Print how many objects are detected in each grid cell\n",
    "    grid_cell_counts = np.zeros((7, 7), dtype=int)\n",
    "    for box in grid_boxes:\n",
    "        grid_col = int(box[\"center_x\"] * 7)\n",
    "        grid_row = int(box[\"center_y\"] * 7)\n",
    "        grid_cell_counts[grid_row, grid_col] += 1\n",
    "    \n",
    "    print(\"\\nObjects detected per grid cell:\")\n",
    "    print(grid_cell_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7951fce",
   "metadata": {},
   "source": [
    "## Arquitectura del Modelo YOLO-like para Detección de Audio\n",
    "\n",
    "Ahora vamos a definir la arquitectura del modelo basada en YOLO (You Only Look Once) para la detección de eventos de audio en espectrogramas. La arquitectura consistirá en bloques convolucionales seguidos de capas fully-connected para generar las predicciones en formato de tensor YOLO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f3d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Bloque convolucional básico: Convolution + BatchNorm + LeakyReLU\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class YOLOAudioModel(nn.Module):\n",
    "    \"\"\"Modelo YOLO para detección de eventos en espectrogramas de audio\"\"\"\n",
    "    def __init__(self, in_channels=1, S=7, B=2, C=3):\n",
    "        \"\"\"\n",
    "        Inicializa el modelo YOLO para audio\n",
    "        \n",
    "        Args:\n",
    "            in_channels: Número de canales de entrada (1 para espectrogramas de audio)\n",
    "            S: Tamaño del grid (S x S)\n",
    "            B: Número de bounding boxes por celda\n",
    "            C: Número de clases\n",
    "        \"\"\"\n",
    "        super(YOLOAudioModel, self).__init__()\n",
    "        \n",
    "        # Número de valores a predecir por celda: B*(x,y,w,h,conf) + C clases\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.output_size = B*5 + C\n",
    "        \n",
    "        # Bloques convolucionales inspirados en YOLOv1 pero más pequeños para espectrogramas\n",
    "        self.layer1 = ConvBlock(in_channels, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # Reduce tamaño a la mitad (128x128)\n",
    "        \n",
    "        self.layer2 = ConvBlock(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # Reduce a 64x64\n",
    "        \n",
    "        self.layer3 = ConvBlock(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # Reduce a 32x32\n",
    "        \n",
    "        self.layer4 = ConvBlock(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)  # Reduce a 16x16\n",
    "        \n",
    "        self.layer5 = ConvBlock(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(2, 2)  # Reduce a 8x8, cercano a nuestro S=7\n",
    "        \n",
    "        # Ajuste final para obtener SxS\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((S, S))\n",
    "        \n",
    "        # Capa de predicción final\n",
    "        self.conv_final = nn.Conv2d(256, self.output_size, kernel_size=1, stride=1, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = self.layer5(x)\n",
    "        x = self.pool5(x)\n",
    "        \n",
    "        # Ajustar al tamaño final del grid\n",
    "        x = self.adaptive_pool(x)\n",
    "        \n",
    "        # Predicciones finales\n",
    "        x = self.conv_final(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Instanciar el modelo para nuestro caso\n",
    "model = YOLOAudioModel(\n",
    "    in_channels=1,  # 1 canal para espectrogramas\n",
    "    S=7,            # Grid de 7x7\n",
    "    B=2,            # 2 bounding boxes por celda\n",
    "    C=len(class_map) # Número de clases según nuestro class_map\n",
    ")\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Resumen del modelo\n",
    "print(f\"\\nResumen del modelo:\")\n",
    "print(f\"- Grid size (S): 7x7\")\n",
    "print(f\"- Bounding boxes per cell (B): 2\")\n",
    "print(f\"- Number of classes (C): {len(class_map)}\")\n",
    "print(f\"- Output tensor shape: (S, S, B*5 + C) = (7, 7, {2*5 + len(class_map)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5344008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOLoss(nn.Module):\n",
    "    \"\"\"Función de pérdida personalizada para modelo YOLO de audio.\n",
    "    \n",
    "    Basada en la función de pérdida original de YOLO que penaliza:\n",
    "    - Error de coordenadas (x, y, w, h)\n",
    "    - Error de confianza (objectness)\n",
    "    - Error de clasificación\n",
    "    \"\"\"\n",
    "    def __init__(self, S=7, B=2, C=3, lambda_coord=5.0, lambda_noobj=0.5):\n",
    "        super(YOLOLoss, self).__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        self.mse = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"Calcula la pérdida YOLO entre predicciones y targets.\n",
    "        \n",
    "        Args:\n",
    "            predictions: Tensor de forma (batch_size, S, S, B*5+C)\n",
    "            targets: Tensor de forma (batch_size, S, S, B*5+C)\n",
    "            \n",
    "        Returns:\n",
    "            Pérdida total\n",
    "        \"\"\"\n",
    "        # Reorganizar predicciones para facilitar el acceso\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.B * 5 + self.C)\n",
    "        \n",
    "        # Extraer componentes\n",
    "        # Para cada bounding box: [x, y, w, h, conf]\n",
    "        pred_boxes = predictions[..., :self.B*5].reshape(-1, self.S, self.S, self.B, 5)\n",
    "        pred_classes = predictions[..., self.B*5:]\n",
    "        \n",
    "        # Igual para los targets\n",
    "        target_boxes = targets[..., :self.B*5].reshape(-1, self.S, self.S, self.B, 5)\n",
    "        target_classes = targets[..., self.B*5:]\n",
    "        \n",
    "        # Máscara para celdas con objetos (confianza > 0)\n",
    "        obj_mask = torch.zeros_like(target_boxes[..., 4])\n",
    "        for b in range(self.B):\n",
    "            obj_mask[..., b] = target_boxes[..., b, 4] > 0\n",
    "        \n",
    "        # 1. Pérdida de coordenadas (solo para celdas con objetos)\n",
    "        xy_loss = self.mse(torch.flatten(pred_boxes[..., :2][obj_mask.bool()]), \n",
    "                           torch.flatten(target_boxes[..., :2][obj_mask.bool()]))\n",
    "        \n",
    "        # Para w, h usamos raíz cuadrada para penalizar menos errores en cajas grandes\n",
    "        wh_pred = torch.sign(pred_boxes[..., 2:4]) * torch.sqrt(torch.abs(pred_boxes[..., 2:4]) + 1e-6)\n",
    "        wh_target = torch.sqrt(target_boxes[..., 2:4] + 1e-6)\n",
    "        wh_loss = self.mse(torch.flatten(wh_pred[obj_mask.bool()]), \n",
    "                          torch.flatten(wh_target[obj_mask.bool()]))\n",
    "        \n",
    "        # 2. Pérdida de confianza\n",
    "        # Para cajas con objetos\n",
    "        conf_obj_loss = self.mse(torch.flatten(pred_boxes[..., 4][obj_mask.bool()]), \n",
    "                               torch.flatten(target_boxes[..., 4][obj_mask.bool()]))\n",
    "        \n",
    "        # Para cajas sin objetos\n",
    "        noobj_mask = ~obj_mask.bool()\n",
    "        conf_noobj_loss = self.mse(torch.flatten(pred_boxes[..., 4][noobj_mask]), \n",
    "                                 torch.flatten(target_boxes[..., 4][noobj_mask]))\n",
    "        \n",
    "        # 3. Pérdida de clasificación (solo para celdas con objetos)\n",
    "        # Crear máscara para celdas con objetos (cualquier caja)\n",
    "        cell_has_obj = obj_mask.sum(dim=3) > 0\n",
    "        class_loss = self.mse(torch.flatten(pred_classes[cell_has_obj]), \n",
    "                             torch.flatten(target_classes[cell_has_obj]))\n",
    "        \n",
    "        # Pérdida total\n",
    "        loss = (\n",
    "            self.lambda_coord * xy_loss + \n",
    "            self.lambda_coord * wh_loss + \n",
    "            conf_obj_loss + \n",
    "            self.lambda_noobj * conf_noobj_loss + \n",
    "            class_loss\n",
    "        )\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# Crear instancia de la función de pérdida\n",
    "yolo_loss = YOLOLoss(S=7, B=2, C=len(class_map))\n",
    "print(\"Función de pérdida personalizada para YOLO creada correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f60357",
   "metadata": {},
   "source": [
    "## Preparación de Datos para Entrenamiento\n",
    "\n",
    "Para entrenar nuestro modelo YOLO para audio, necesitamos preparar los datos de entrenamiento. Convertiremos nuestros espectrogramas en tensores PyTorch y dividiremos el conjunto de datos en entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147ffb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class AudioSpectrogramDataset(Dataset):\n",
    "    \"\"\"Dataset para espectrogramas de audio y sus etiquetas YOLO\"\"\"\n",
    "    def __init__(self, clips, tensors, max_freq_hz=24000.0, transform=None):\n",
    "        self.clips = clips\n",
    "        self.tensors = tensors\n",
    "        self.max_freq_hz = max_freq_hz\n",
    "        self.transform = transform\n",
    "        # Lista de nombres de clips para acceso por índice\n",
    "        self.clip_names = list(self.tensors.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.clip_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        clip_name = self.clip_names[idx]\n",
    "        # Obtener el tensor YOLO para este clip\n",
    "        yolo_tensor = self.tensors[clip_name]\n",
    "        \n",
    "        # Buscar el objeto AudioClip correspondiente\n",
    "        clip = next((c for c in self.clips if c.clip_name == clip_name), None)\n",
    "        \n",
    "        if clip is None:\n",
    "            raise ValueError(f\"No se encontró el objeto AudioClip para {clip_name}\")\n",
    "        \n",
    "        # Cargar el audio y convertirlo a espectrograma\n",
    "        audio_data, sr = load_and_extract_clip(clip)\n",
    "        \n",
    "        # Calcular el espectrograma usando librosa\n",
    "        D = librosa.stft(audio_data)\n",
    "        # Convertir a magnitud en dB\n",
    "        D_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "        \n",
    "        # Asegurarnos de tener un tamaño consistente para el modelo\n",
    "        # Usar tamaño 256x256 para la entrada de la red\n",
    "        D_resized = librosa.util.fix_length(D_db, size=256, axis=1)\n",
    "        D_resized = librosa.util.fix_length(D_resized, size=256, axis=0)\n",
    "        \n",
    "        # Normalizar los valores al rango [0, 1]\n",
    "        D_norm = (D_resized - D_resized.min()) / (D_resized.max() - D_resized.min() + 1e-8)\n",
    "        \n",
    "        # Convertir a tensor de PyTorch y agregar dimensión de canal\n",
    "        spectrogram = torch.tensor(D_norm, dtype=torch.float32).unsqueeze(0)  # [1, 256, 256]\n",
    "        \n",
    "        # Aplicar transformaciones adicionales si existen\n",
    "        if self.transform:\n",
    "            spectrogram = self.transform(spectrogram)\n",
    "        \n",
    "        # Convertir el tensor YOLO a tensor de PyTorch\n",
    "        yolo_target = torch.tensor(yolo_tensor, dtype=torch.float32)\n",
    "        \n",
    "        return spectrogram, yolo_target\n",
    "\n",
    "# Dividir los clips en conjuntos de entrenamiento y prueba\n",
    "clip_names = list(all_tensors.keys())\n",
    "random.seed(42)  # Para reproducibilidad\n",
    "\n",
    "train_names, test_names = train_test_split(clip_names, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Total de clips: {len(clip_names)}\")\n",
    "print(f\"Clips para entrenamiento: {len(train_names)}\")\n",
    "print(f\"Clips para prueba: {len(test_names)}\")\n",
    "\n",
    "# Crear diccionarios separados para entrenamiento y prueba\n",
    "train_tensors = {name: all_tensors[name] for name in train_names}\n",
    "test_tensors = {name: all_tensors[name] for name in test_names}\n",
    "\n",
    "# Crear los datasets\n",
    "train_dataset = AudioSpectrogramDataset(\n",
    "    clips=all_clips,\n",
    "    tensors=train_tensors,\n",
    "    max_freq_hz=24000.0\n",
    ")\n",
    "\n",
    "test_dataset = AudioSpectrogramDataset(\n",
    "    clips=all_clips,\n",
    "    tensors=test_tensors,\n",
    "    max_freq_hz=24000.0\n",
    ")\n",
    "\n",
    "# Crear dataloaders\n",
    "batch_size = 8  # Tamaño de batch pequeño debido a la complejidad del modelo\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=4  # Ajusta según tu CPU\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Visualizar un batch de datos\n",
    "print(\"\\nVisualización de un batch de datos:\")\n",
    "for images, targets in train_dataloader:\n",
    "    print(f\"Batch de espectrogramas: {images.shape}\")\n",
    "    print(f\"Batch de tensores YOLO: {targets.shape}\")\n",
    "    break  # Solo el primer batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
